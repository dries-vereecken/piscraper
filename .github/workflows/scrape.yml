name: Scrape and write to Postgres

on:
  schedule:
    # Run 3 times per day: 6 AM, 12 PM, 6 PM UTC
    - cron: "0 6,12,18 * * *"
  workflow_dispatch:
    # Allow manual triggering

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    env:
      DATABASE_URL: ${{ secrets.DATABASE_URL }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install Chrome
        run: |
          sudo apt-get update
          sudo apt-get install -y wget gnupg
          wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
          sudo sh -c 'echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" >> /etc/apt/sources.list.d/google-chrome.list'
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable

      - name: Install ChromeDriver
        run: |
          CHROME_VERSION=$(google-chrome --version | cut -d ' ' -f3 | cut -d '.' -f1)
          CHROMEDRIVER_VERSION=$(curl -s "https://chromedriver.storage.googleapis.com/LATEST_RELEASE_${CHROME_VERSION}")
          wget -N "https://chromedriver.storage.googleapis.com/${CHROMEDRIVER_VERSION}/chromedriver_linux64.zip"
          unzip chromedriver_linux64.zip
          sudo mv chromedriver /usr/local/bin/chromedriver
          sudo chmod +x /usr/local/bin/chromedriver

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r "Schedule scraper/requirements.txt"

      - name: Test database connection
        run: |
          cd "Schedule scraper"
          python -c "from db_utils import test_connection; test_connection()"

      - name: Run CoolCharm scraper
        run: |
          cd "Schedule scraper"
          python scraper_coolcharm.py
        continue-on-error: true

      - name: Run Koepel scraper
        run: |
          cd "Schedule scraper"
          python scraper_koepel.py
        continue-on-error: true

      - name: Run Rite scraper
        run: |
          cd "Schedule scraper"
          python scraper_rite.py
        continue-on-error: true

      - name: Run RowReformer scraper
        run: |
          cd "Schedule scraper"
          python scraper_rowreformer.py
        continue-on-error: true

      - name: Run Silver Aggregation
        run: |
          cd "Schedule scraper"
          python silver_aggregation.py
        continue-on-error: false

      - name: Summary
        run: |
          echo "Scraping and aggregation completed!"
          echo "✅ Bronze layer: Raw scraping data written to schedule_snapshots"
          echo "✅ Silver layer: Aggregated classes written to silver_classes"
          echo "Check the job logs above for any errors or issues."
