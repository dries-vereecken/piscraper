name: Scrape and write to Postgres

on:
  schedule:
    # Run 3 times per day: 6 AM, 12 PM, 6 PM UTC
    - cron: "0 6,12,18 * * *"
  workflow_dispatch:
    # Allow manual triggering

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    env:
      DATABASE_URL: ${{ secrets.DATABASE_URL }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Setup Chrome
        run: |
          wget -q https://dl.google.com/linux/chrome/deb/pool/main/g/google-chrome-stable/google-chrome-stable_136.0.7103.92-1_amd64.deb
          sudo dpkg -i google-chrome-stable_136.0.7103.92-1_amd64.deb
          google-chrome --version
      
      - name: Setup ChromeDriver
        run: |
          # Get Chrome version
          CHROME_VERSION=$(google-chrome --version | awk '{print $3}' | cut -d'.' -f1)
          echo "Chrome version: $CHROME_VERSION"
          
          # Download and install ChromeDriver
          wget -q "https://storage.googleapis.com/chrome-for-testing-public/136.0.7103.92/linux64/chromedriver-linux64.zip"
          unzip -q chromedriver-linux64.zip
          sudo mv chromedriver-linux64/chromedriver /usr/local/bin/
          sudo chmod +x /usr/local/bin/chromedriver
          
          # Verify installation
          echo "ChromeDriver version:"
          chromedriver --version

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Test database connection
        run: |
          python -c "from db_utils import test_connection; test_connection()"

      - name: Run CoolCharm scraper
        run: |
          python scraper_coolcharm.py
        continue-on-error: true

      - name: Run Koepel scraper
        run: |
          python scraper_koepel.py
        continue-on-error: true

      - name: Run Rite scraper
        run: |
          python scraper_rite.py
        continue-on-error: true

      - name: Run RowReformer scraper
        run: |
          python scraper_rowreformer.py
        continue-on-error: true

      - name: Run Silver Aggregation
        run: |
          python silver_aggregation.py
        continue-on-error: false

      - name: Summary
        run: |
          echo "Scraping and aggregation completed!"
          echo "✅ Bronze layer: Raw scraping data written to schedule_snapshots"
          echo "✅ Silver layer: Aggregated classes written to silver_classes"
          echo "Check the job logs above for any errors or issues."
